{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n",
    "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import Process, Pipe\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Environment (for multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(Process):\n",
    "    def __init__(self, env_idx, child_conn, env_name, state_size, action_size, visualize=False):\n",
    "        super(Environment, self).__init__()\n",
    "        self.env = gym.make(env_name)\n",
    "        self.is_render = visualize\n",
    "        self.env_idx = env_idx\n",
    "        self.child_conn = child_conn\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def run(self):\n",
    "        super(Environment, self).run()\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        self.child_conn.send(state)\n",
    "        while True:\n",
    "            action = self.child_conn.recv()\n",
    "            if self.is_render and self.env_idx == 0:\n",
    "                self.env.render()\n",
    "\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            self.child_conn.send([state, reward, done, info])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "        # creating Neural Net for actor model\n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"sigmoid\")(X) # here we choose a sigmoid activation as the action space is only 1d with 1 or 0\n",
    "\n",
    "        #just compiling the model with layers for model.fit later\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
    "        print('model compiled successfully')\n",
    "\n",
    "\n",
    "    # computing the custom PPO Loss for the actor according to the formula of PPO\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        ENTROPY_LOSS = 0.001\n",
    "        \n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * prediction_picks\n",
    "\n",
    "        prob = K.clip(prob, 1e-10, 1.0)\n",
    "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "        \n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    # ???\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        old_values = Input(shape=(1,))\n",
    "\n",
    "        V = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "        V = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "        V = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n",
    "        self.Critic.compile(loss=[self.critic_PPO2_loss(old_values)], optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def critic_PPO2_loss(self, values):\n",
    "        def loss(y_true, y_pred):\n",
    "            LOSS_CLIPPING = 0.2\n",
    "            clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "            v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "            v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "            value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "            #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "            return value_loss\n",
    "        return loss\n",
    "\n",
    "    #???\n",
    "    def predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Agent(Actor + Critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PPOAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.EPISODES = 1000 # total episodes to train through all environments\n",
    "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n",
    "        self.max_average = 0 # when average score is above 0 model will be saved\n",
    "        self.lr = 0.00025\n",
    "        self.epochs = 10 # training epochs\n",
    "        self.shuffle=False\n",
    "        self.Training_batch = 1000\n",
    "        #self.optimizer = RMSprop\n",
    "        self.optimizer = Adam\n",
    "        print(self.env.action_space)\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter(comment=\"_\"+self.env_name+\"_\"+self.optimizer.__name__+\"_\"+str(self.lr))\n",
    "        \n",
    "        # Instantiate plot memory\n",
    "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n",
    "\n",
    "        # Create Actor-Critic network models\n",
    "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        \n",
    "        self.Actor_name = f\"{self.env_name}_PPO_Actor.h5\"\n",
    "        self.Critic_name = f\"{self.env_name}_PPO_Critic.h5\"\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\" example:\n",
    "        pred = np.array([0.05, 0.85, 0.1])\n",
    "        action_size = 3\n",
    "        np.random.choice(a, p=pred)\n",
    "        result>>> 1, because it have the highest probability to be taken\n",
    "        \"\"\"\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(state)[0]\n",
    "        print(prediction)\n",
    "        # take an action from the action space with the probabilites given by the Actor Model Prediction for the next step\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        # use a onehot to actually choose the action\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        print(action)\n",
    "        return action, action_onehot, prediction\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        # We apply the discount and normalize it to avoid big variability of rewards\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "\n",
    "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "        #print actions for debugging\n",
    "        # print(states)\n",
    "        # print(next_states)\n",
    "        print('actions:')\n",
    "        print(actions)\n",
    "        print('next_states')\n",
    "        print(next_states)\n",
    "\n",
    "        \n",
    "\n",
    "        # Get Critic network predictions \n",
    "        values = self.Critic.predict(states)\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        discounted_r = self.discount_rewards(rewards)\n",
    "        advantages = np.vstack(discounted_r - values)\n",
    "\n",
    "        # advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "\n",
    "        '''\n",
    "        pylab.plot(advantages,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        # pack all advantages, predictions and actions to y_true and when they are received\n",
    "        # in custom PPO loss function we unpack it\n",
    "        y_true = np.hstack([advantages, predictions, actions])\n",
    "        print('y_true:')\n",
    "        print(y_true)\n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "\n",
    "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        self.replay_count += 1\n",
    " \n",
    "    def load(self):\n",
    "        self.Actor.Actor.load_weights(self.Actor_name)\n",
    "        self.Critic.Critic.load_weights(self.Critic_name)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.Actor.save_weights(self.Actor_name)\n",
    "        self.Critic.Critic.save_weights(self.Critic_name)\n",
    "        \n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores_.append(score)\n",
    "        self.episodes_.append(episode)\n",
    "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes_, self.scores_, 'b')\n",
    "            pylab.plot(self.episodes_, self.average_, 'r')\n",
    "            pylab.title(self.env_name+\" PPO training cycle\", fontsize=18)\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.grid(True)\n",
    "                pylab.savefig(self.env_name+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "        # saving best models\n",
    "        if self.average_[-1] >= self.max_average:\n",
    "            self.max_average = self.average_[-1]\n",
    "            self.save()\n",
    "            SAVING = \"SAVING\"\n",
    "            # decreaate learning rate every saved model\n",
    "            self.lr *= 0.95\n",
    "            K.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n",
    "            K.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n",
    "        else:\n",
    "            SAVING = \"\"\n",
    "\n",
    "        return self.average_[-1], SAVING\n",
    "    \n",
    "    def run(self): # train only when episode is finished\n",
    "        state = self.env.reset()\n",
    "        #???\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        # setting break parameters\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        # looping\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "                    \n",
    "                    self.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "\n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "        self.env.close()\n",
    "\n",
    "    def run_batch(self): # train every self.Training_batch episodes\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            for t in range(self.Training_batch):\n",
    "                self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "\n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "                    \n",
    "            self.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "        self.env.close()  \n",
    "\n",
    "        \n",
    "    def run_multiprocesses(self, num_worker = 4):\n",
    "        works, parent_conns, child_conns = [], [], []\n",
    "        for idx in range(num_worker):\n",
    "            parent_conn, child_conn = Pipe()\n",
    "            work = Environment(idx, child_conn, self.env_name, self.state_size[0], self.action_size, True)\n",
    "            work.start()\n",
    "            works.append(work)\n",
    "            parent_conns.append(parent_conn)\n",
    "            child_conns.append(child_conn)\n",
    "\n",
    "        states =        [[] for _ in range(num_worker)]\n",
    "        next_states =   [[] for _ in range(num_worker)]\n",
    "        actions =       [[] for _ in range(num_worker)]\n",
    "        rewards =       [[] for _ in range(num_worker)]\n",
    "        dones =         [[] for _ in range(num_worker)]\n",
    "        predictions =   [[] for _ in range(num_worker)]\n",
    "        score =         [0 for _ in range(num_worker)]\n",
    "\n",
    "        state = [0 for _ in range(num_worker)]\n",
    "        for worker_id, parent_conn in enumerate(parent_conns):\n",
    "            state[worker_id] = parent_conn.recv()\n",
    "\n",
    "        while self.episode < self.EPISODES:\n",
    "            predictions_list = self.Actor.predict(np.reshape(state, [num_worker, self.state_size[0]]))\n",
    "            actions_list = [np.random.choice(self.action_size, p=i) for i in predictions_list]\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                parent_conn.send(actions_list[worker_id])\n",
    "                action_onehot = np.zeros([self.action_size])\n",
    "                action_onehot[actions_list[worker_id]] = 1\n",
    "                actions[worker_id].append(action_onehot)\n",
    "                predictions[worker_id].append(predictions_list[worker_id])\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                next_state, reward, done, _ = parent_conn.recv()\n",
    "\n",
    "                states[worker_id].append(state[worker_id])\n",
    "                next_states[worker_id].append(next_state)\n",
    "                rewards[worker_id].append(reward)\n",
    "                dones[worker_id].append(done)\n",
    "                state[worker_id] = next_state\n",
    "                score[worker_id] += reward\n",
    "\n",
    "                if done:\n",
    "                    average, SAVING = self.PlotModel(score[worker_id], self.episode)\n",
    "                    print(\"episode: {}/{}, worker: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, worker_id, score[worker_id], average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/score_per_episode', score[worker_id], self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/learning_rate', self.lr, self.episode)\n",
    "                    score[worker_id] = 0\n",
    "                    if(self.episode < self.EPISODES):\n",
    "                        self.episode += 1\n",
    "                        \n",
    "            for worker_id in range(num_worker):\n",
    "                if len(states[worker_id]) >= self.Training_batch:\n",
    "                    self.replay(states[worker_id], actions[worker_id], rewards[worker_id], predictions[worker_id], dones[worker_id], next_states[worker_id])\n",
    "                    \n",
    "                    states[worker_id] = []\n",
    "                    next_states[worker_id] = []\n",
    "                    actions[worker_id] = []\n",
    "                    rewards[worker_id] = []\n",
    "                    dones[worker_id] = []\n",
    "                    predictions[worker_id] = []\n",
    "\n",
    "        # terminating processes after while loop\n",
    "        works.append(work)\n",
    "        for work in works:\n",
    "            work.terminate()\n",
    "            print('TERMINATED:', work)\n",
    "            work.join()\n",
    "            \n",
    "\n",
    "    def test(self, test_episodes = 100):\n",
    "        self.load()\n",
    "        for e in range(100):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size[0]])\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state)[0])\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, test_episodes, score))\n",
    "                    break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2022-03-15 10:51:32.279415: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.346383: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.396257: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50000226 0.5000012 ]\n",
      "0\n",
      "[0.4999963 0.5000236]\n",
      "0\n",
      "[0.4999953  0.50004816]\n",
      "1\n",
      "[0.4999981 0.5000234]\n",
      "0\n",
      "[0.49999756 0.5000483 ]\n",
      "0\n",
      "[0.49999756 0.5000725 ]\n",
      "0\n",
      "[0.4999982  0.50009644]\n",
      "0\n",
      "[0.49999952 0.5001202 ]\n",
      "0\n",
      "[0.5000016  0.50014395]\n",
      "0\n",
      "[0.5000046 0.5001676]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 10:51:32.484767: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.500756: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.515901: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.530265: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.543465: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.555955: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.568460: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.581343: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.642280: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.697543: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.712755: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.774280: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.827101: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.839964: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-15 10:51:32.895909: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/1000, score: 10.0, average: 10.00 SAVING\n",
      "actions:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "next_states\n",
      "[[ 3.2973144e-02 -1.8596746e-01  4.2689286e-02  2.8437832e-01]\n",
      " [ 2.9253796e-02 -3.8167146e-01  4.8376855e-02  5.9021372e-01]\n",
      " [ 2.1620367e-02 -1.8725905e-01  6.0181126e-02  3.1315351e-01]\n",
      " [ 1.7875187e-02 -3.8318437e-01  6.6444196e-02  6.2419206e-01]\n",
      " [ 1.0211499e-02 -5.7916796e-01  7.8928038e-02  9.3703896e-01]\n",
      " [-1.3718606e-03 -7.7526045e-01  9.7668819e-02  1.2534428e+00]\n",
      " [-1.6877070e-02 -9.7148824e-01  1.2273767e-01  1.5750506e+00]\n",
      " [-3.6306836e-02 -1.1678411e+00  1.5423869e-01  1.9033586e+00]\n",
      " [-5.9663657e-02 -1.3642579e+00  1.9230586e-01  2.2396529e+00]\n",
      " [-8.6948819e-02 -1.5606103e+00  2.3709892e-01  2.5849421e+00]]\n",
      "y_true:\n",
      "[[ 1.50812489  1.17379636  0.83609078  0.49497402  0.15041165 -0.19763115\n",
      "  -0.54918954 -0.90429902 -1.26299547 -1.62531511  0.50000226  0.50000119\n",
      "   0.        ]\n",
      " [ 1.34120306  1.00687453  0.66916895  0.3280522  -0.01651017 -0.36455298\n",
      "  -0.71611136 -1.07122084 -1.42991729 -1.79223693  0.4999963   0.5000236\n",
      "   0.        ]\n",
      " [ 1.09842521  0.76409668  0.4263911   0.08527435 -0.25928803 -0.60733083\n",
      "  -0.95888922 -1.3139987  -1.67269514 -2.03501479  0.49999529  0.50004816\n",
      "   1.        ]\n",
      " [ 1.3338023   0.99947377  0.66176818  0.32065143 -0.02391094 -0.37195374\n",
      "  -0.72351213 -1.07862161 -1.43731806 -1.7996377   0.49999809  0.50002342\n",
      "   0.        ]\n",
      " [ 1.08183008  0.74750155  0.40979597  0.06867922 -0.27588315 -0.62392596\n",
      "  -0.97548434 -1.33059382 -1.68929027 -2.05160991  0.49999756  0.50004828\n",
      "   0.        ]\n",
      " [ 0.84176504  0.50743652  0.16973093 -0.17138582 -0.51594819 -0.86399099\n",
      "  -1.21554938 -1.57065886 -1.92935531 -2.29167495  0.49999756  0.50007248\n",
      "   0.        ]\n",
      " [ 0.60141051  0.26708198 -0.0706236  -0.41174036 -0.75630273 -1.10434553\n",
      "  -1.45590392 -1.8110134  -2.16970985 -2.53202949  0.49999821  0.50009644\n",
      "   0.        ]\n",
      " [ 0.35880464  0.02447611 -0.31322947 -0.65434622 -0.99890859 -1.3469514\n",
      "  -1.69850978 -2.05361926 -2.41231571 -2.77463535  0.49999952  0.50012022\n",
      "   0.        ]\n",
      " [ 0.11168462 -0.22264391 -0.56034949 -0.90146624 -1.24602862 -1.59407142\n",
      "  -1.94562981 -2.30073929 -2.65943573 -3.02175537  0.50000161  0.50014395\n",
      "   0.        ]\n",
      " [-0.13819331 -0.47252184 -0.81022742 -1.15134417 -1.49590655 -1.84394935\n",
      "  -2.19550774 -2.55061722 -2.90931366 -3.2716333   0.50000459  0.50016761\n",
      "   0.        ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (10, 13) was passed for an output of shape (None, 2) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_12183/3857796114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_12183/1516024177.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Workers:{1}/learning_rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_12183/1516024177.py\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, states, actions, rewards, predictions, dones, next_states)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# training Actor and Critic networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0ma_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     return func.fit(\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m                                                      steps_per_epoch, x)\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m     x, y, sample_weights = model._standardize_user_data(\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2345\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2347\u001b[0;31m     return self._standardize_tensors(\n\u001b[0m\u001b[1;32m   2348\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m           training_utils_v1.check_loss_and_target_compatibility(\n\u001b[0m\u001b[1;32m   2459\u001b[0m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[1;32m   2460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0mloss_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_loss_wrapper\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0mloss_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0m\u001b[1;32m    833\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (10, 13) was passed for an output of shape (None, 2) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_name = 'CartPole-v1'\n",
    "    agent = PPOAgent(env_name=env_name)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
