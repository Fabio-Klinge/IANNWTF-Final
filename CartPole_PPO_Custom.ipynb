{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "class Trajectory_Storage:\n",
    "    # T for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # T initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "\n",
    "        self.l = [\n",
    "            Dense(64, activation=\"tanh\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.02)),\n",
    "            Dense(64, activation=\"tanh\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.02)),\n",
    "            Dense(2, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.02))\n",
    "        ]\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "#@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "   # tf.print(type(logits))\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "   # tf.print(action)\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l = [\n",
    "            Dense(64, activation=\"tanh\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.02)),\n",
    "            Dense(64, activation=\"tanh\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.02)),\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.02))\n",
    "        ]\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Policy Function\n",
    "\n",
    "Training the Actor Model Using the typical PPO-Clipping Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    # print(\"Policy grads: \")\n",
    "    # print(policy_grads)\n",
    "    # print(\"Actor Variables:\")\n",
    "    # print([actor.trainable_variables])\n",
    "    # print(type(policy_grads), type(actor.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    optimizer_2.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Hyperparameters\n",
    "epochs = 40\n",
    "steps_per_epoch = 4000\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "clip_ratio = 0.2\n",
    "target_kl = 0.01\n",
    "optimizer = Adam()\n",
    "optimizer_2 = Adam()\n",
    "\n",
    "render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Inits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# define environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# get observation_dims and amount of possible actions (1 for CartPole-v1)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage for observations, actions, rewards etc during trajectory\n",
    "T = Trajectory_Storage(observation_dimensions=observation_dimensions, size=steps_per_epoch)\n",
    "\n",
    "# init the actor and critics model\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "mean_returns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:32<00:00, 123.54it/s]\n",
      "2022-03-28 16:34:20.162395: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-28 16:34:20.452723: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 1. Mean Return: 22.471910112359552. Mean Length: 22.471910112359552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 142.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 2. Mean Return: 30.76923076923077. Mean Length: 30.76923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:26<00:00, 150.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 3. Mean Return: 43.956043956043956. Mean Length: 43.956043956043956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 141.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 4. Mean Return: 86.95652173913044. Mean Length: 86.95652173913044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:36<00:00, 108.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 5. Mean Return: 125.0. Mean Length: 125.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:29<00:00, 137.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 6. Mean Return: 181.8181818181818. Mean Length: 181.8181818181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 140.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 7. Mean Return: 235.2941176470588. Mean Length: 235.2941176470588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:37<00:00, 106.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 8. Mean Return: 210.52631578947367. Mean Length: 210.52631578947367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:25<00:00, 156.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 9. Mean Return: 307.6923076923077. Mean Length: 307.6923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:26<00:00, 153.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 10. Mean Return: 363.6363636363636. Mean Length: 363.6363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:27<00:00, 146.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 11. Mean Return: 266.6666666666667. Mean Length: 266.6666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:34<00:00, 116.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 12. Mean Return: 210.52631578947367. Mean Length: 210.52631578947367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:26<00:00, 151.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 13. Mean Return: 363.6363636363636. Mean Length: 363.6363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:26<00:00, 150.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 14. Mean Return: 307.6923076923077. Mean Length: 307.6923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:37<00:00, 105.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 15. Mean Return: 363.6363636363636. Mean Length: 363.6363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:27<00:00, 144.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 16. Mean Return: 400.0. Mean Length: 400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 138.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 17. Mean Return: 333.3333333333333. Mean Length: 333.3333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:33<00:00, 121.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 18. Mean Return: 363.6363636363636. Mean Length: 363.6363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:30<00:00, 131.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 19. Mean Return: 400.0. Mean Length: 400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 139.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 20. Mean Return: 444.44444444444446. Mean Length: 444.44444444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 141.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 21. Mean Return: 363.6363636363636. Mean Length: 363.6363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:35<00:00, 112.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 22. Mean Return: 333.3333333333333. Mean Length: 333.3333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:27<00:00, 142.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 23. Mean Return: 500.0. Mean Length: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:27<00:00, 142.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 24. Mean Return: 500.0. Mean Length: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:35<00:00, 112.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 25. Mean Return: 444.44444444444446. Mean Length: 444.44444444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 139.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 26. Mean Return: 363.6363636363636. Mean Length: 363.6363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 140.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 27. Mean Return: 400.0. Mean Length: 400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 140.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 28. Mean Return: 400.0. Mean Length: 400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:36<00:00, 108.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 29. Mean Return: 400.0. Mean Length: 400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:27<00:00, 147.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 30. Mean Return: 333.3333333333333. Mean Length: 333.3333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:25<00:00, 156.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 31. Mean Return: 444.44444444444446. Mean Length: 444.44444444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:33<00:00, 119.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 32. Mean Return: 500.0. Mean Length: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:25<00:00, 158.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 33. Mean Return: 444.44444444444446. Mean Length: 444.44444444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:26<00:00, 148.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 34. Mean Return: 400.0. Mean Length: 400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:29<00:00, 135.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 35. Mean Return: 444.44444444444446. Mean Length: 444.44444444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:32<00:00, 123.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 36. Mean Return: 500.0. Mean Length: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 141.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 37. Mean Return: 500.0. Mean Length: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 140.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 38. Mean Return: 363.6363636363636. Mean Length: 363.6363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:35<00:00, 113.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 39. Mean Return: 400.0. Mean Length: 400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:28<00:00, 138.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 40. Mean Return: 444.44444444444446. Mean Length: 444.44444444444446\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "     # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in tqdm(range(steps_per_epoch)):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        #print(observation)\n",
    "        observation = observation.reshape(1, -1)\n",
    "        \n",
    "        #print(observation)\n",
    "        logits, action = sample_action(observation)\n",
    "        #print(logits)\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "        #print(observation)\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        T.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            T.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = T.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )\n",
    "    mean_returns.append(sum_return/num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(epochs), mean_returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:28<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 episodes 47659.0 / 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test the trained variable \n",
    "\n",
    "test_length = 100\n",
    "passed = []\n",
    "\n",
    "observation= env.reset()\n",
    "reward_sum = 0\n",
    "done \n",
    "for i in tqdm(range(test_length)):\n",
    "    observation = env.reset()\n",
    "    terminal = False\n",
    "    done = False\n",
    "    reward_sum = 0\n",
    "\n",
    "    while not terminal:\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "        observation = observation.reshape(1, -1)\n",
    "            \n",
    "        logits, action = sample_action(observation)\n",
    "        \n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "        reward_sum += reward\n",
    "        terminal = done \n",
    "        observation = observation_new\n",
    "   \n",
    "   \n",
    "    passed.append(reward_sum)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Average reward over 100 episodes {sum(passed)} / {test_length}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
