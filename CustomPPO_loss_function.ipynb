{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self):\n",
    "        # init creating a storage for all observation variables during trajectory\n",
    "        self.observations = []\n",
    "        # create arrays for chosen actions and rewards\n",
    "        self.actions = []\n",
    "        self.logits = []\n",
    "        self.rewards = []\n",
    "        self.BaselineEstimate = []\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self,observation, action, logits, reward, BaselineEstimate):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) # value of critics network\n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        # append all already stored values to finished episodes\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate, \n",
    "             sum(self.rewards)]) # get the return of the whole episode\n",
    "             \n",
    "        # empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        # create arrays for chosen actions and rewards\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "\n",
    "    # return array of finished trajectories stored in self.episodes and the amount of episodes\n",
    "    def get_episodes(self):\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "\n",
    "        self.l = [\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(4, activation=\"softmax\")\n",
    "        ]\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "#@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "   # tf.print(type(logits))\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "   # tf.print(action)\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l = [\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Hyperparameters\n",
    "epochs = 1\n",
    "steps_per_epoch = 1000 # ~10 Episodes per epoch, then compute new parameters (smaller batching)\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-4\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "clip_ratio = 0.2\n",
    "target_kl = 0.01\n",
    "optimizer = Adam()\n",
    "\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# define environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# get observation_dims and amount of possible actions (1 for CartPole-v1)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage for observations, actions, rewards etc during trajectory\n",
    "T = Storage()\n",
    "\n",
    "# init the actor and critics model\n",
    "observation_input = Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 280.08it/s]\n"
     ]
    }
   ],
   "source": [
    "episodes_total = 0\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    for t in tqdm(range(steps_per_epoch)):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        observation = observation.reshape(1,-1)\n",
    "\n",
    "        # obtain action and logits for this observation by our actor\n",
    "        logits, action = sample_action(observation=observation)\n",
    "        \n",
    "        # make a step in environment and obtain the rewards for it\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "        # sum up rewards over this episode and count amount of frames\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # get the Base-Estimate from the Critics network\n",
    "        base_estimate = critic(observation)\n",
    "\n",
    "        # store Variables collected in this step\n",
    "        T.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "        #update the observations\n",
    "        observation = observation_new\n",
    "        # check if terminal state is reached in env, if so save episode and refresh storage, reset env\n",
    "        if done:\n",
    "            T.conclude_episode()\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # obtain all episodes saved in storage\n",
    "    episodes, amount_episodes = T.get_episodes()\n",
    "\n",
    "  \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Episodes = 10\n",
      "[-0.4790546468764194, 1.6024155095453068, -0.8532222435471215, -2.5304715012188965, -1.6946305504659438, 2.236095066822327, -2.1760836749764962, -2.467359432374819, 2.9053223620032727, 1.4876082282435845, -1.990866033047098, -2.022995974206111, 1.4645913421566206, -2.212581998883222, -1.7352526075670756, -1.9677994285837872, -0.5094523280217402, 0.5343437857893718, -2.2335737105127578, -2.1834460207286086, 2.130561208901111, 2.298631632959837, 0.7327982247573288, -2.449238524607125, -2.4901511196217427, 0.26669237567527376, -2.2859865631965874, -2.2279349523033147, -2.3305846320511976, -2.1585738872512352, -2.1506324638698957, -2.023824062795056, -1.9005912990468516, 0.6577599112616894, -2.1124023881890808, -2.225041217963222, 1.767636115434226, 1.4475674083359251, -1.8783990707850353, -1.9123640750072468, 3.532896661745025, 2.9337701293436966, -1.6234852695887116, -1.646085711590331, -1.3082425922829646, -1.339505249615371, -1.6495327781140066, -1.749509864558944, 3.540097282110378, -1.9556707656110734, 3.7635786584860513, -2.3569885409254128, -2.0944055860948083, -2.3324174385114973, -2.5453480612985673, -2.019612796921193, -1.7807739272342633, -1.8085358135938066, 0.4314627376146916, -2.1772748435558626, 2.746356695448509, -2.40085710530053, -2.6376087189216535, -2.6835376911741933, -1.9614709640389936, -2.1492025566311383, 3.637250705539242, -2.202284240558413, -2.7964177582656746, -2.9152100155172023, 2.4961110846472083, -2.5844212469563104, -3.2632917286312293, -2.6857093481589516, 0.5071539827413403, -1.779713378767542, -3.368104063481097, -2.7399278372922993, -0.08703520205948506, 0.8606337983751018, -2.2373320220547144, -3.5830150858225793, -1.9209448304878538, -0.5313462032508653, -3.6031937772748948]\n"
     ]
    }
   ],
   "source": [
    "# episodes [episode][particular values // 0: Observations, 1: actions, 2: logits, 3, rewards, 4: BaselineEstimates from Critics]\n",
    "#print(episodes[0][4])\n",
    "print(f'Number of Episodes = {amount_episodes}')\n",
    "\n",
    "\n",
    "\n",
    "### Advantagefunction\n",
    "\n",
    "# estimated Value of the current situtation from the critics network\n",
    "b_estimates = episodes[0][4] \n",
    "# for i in b_estimates:\n",
    "#   print(i.numpy())\n",
    "\n",
    "# Discounted sum of rewards\n",
    "print(episodes[0][3]) \n",
    "rewards = episodes[0][3]\n",
    "gamma = 0.99\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward(rewards, gamma):\n",
    "    i = 0\n",
    "    discounted_rewards = []\n",
    "    for r in rewards:\n",
    "        disc = 0\n",
    "        for t in rewards[i:-1]:\n",
    "            discount_t = gamma ** t\n",
    "            disc += t * discount_t\n",
    "        i += 1\n",
    "        discounted_rewards.append(disc)\n",
    "    return discounted_rewards\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.61598146e+01 -8.56784459e+01 -8.72552626e+01 -8.63946900e+01\n",
      " -8.37990386e+01 -8.20752969e+01 -8.42617004e+01 -8.20375001e+01\n",
      " -7.95081913e+01 -8.23299072e+01 -8.37954404e+01 -8.17643374e+01\n",
      " -7.96997887e+01 -8.11429801e+01 -7.88806456e+01 -7.71148630e+01\n",
      " -7.51077559e+01 -7.45956882e+01 -7.51271704e+01 -7.28428888e+01\n",
      " -7.06109947e+01 -7.26964210e+01 -7.49425600e+01 -7.56699815e+01\n",
      " -7.31597033e+01 -7.06064434e+01 -7.08724218e+01 -6.85333052e+01\n",
      " -6.62549188e+01 -6.38690956e+01 -6.16631823e+01 -5.94655550e+01\n",
      " -5.74001447e+01 -5.54628984e+01 -5.61163243e+01 -5.39585908e+01\n",
      " -5.16832275e+01 -5.34197389e+01 -5.48463993e+01 -5.29322033e+01\n",
      " -5.09827265e+01 -5.43923838e+01 -5.72409148e+01 -5.55907229e+01\n",
      " -5.39171762e+01 -5.25916194e+01 -5.12339576e+01 -4.95568472e+01\n",
      " -4.77762991e+01 -5.11926587e+01 -4.91981648e+01 -5.28220460e+01\n",
      " -5.04085543e+01 -4.82695943e+01 -4.58818530e+01 -4.32705483e+01\n",
      " -4.12095245e+01 -3.93965948e+01 -3.75548861e+01 -3.79844833e+01\n",
      " -3.57590371e+01 -3.84306267e+01 -3.59711319e+01 -3.32626659e+01\n",
      " -3.05057645e+01 -2.85052442e+01 -2.63091133e+01 -2.98158043e+01\n",
      " -2.75642313e+01 -2.46881023e+01 -2.16862142e+01 -2.41204863e+01\n",
      " -2.14680559e+01 -1.80959603e+01 -1.53367706e+01 -1.58413512e+01\n",
      " -1.40295239e+01 -1.05454570e+01 -7.72903378e+00 -7.64192736e+00\n",
      " -8.49515667e+00 -6.20695034e+00 -2.49256098e+00 -5.34173351e-01\n",
      "  1.40966968e-05]\n"
     ]
    }
   ],
   "source": [
    "# get discounted sum of rewards \n",
    "disc_sum = discounted_reward(rewards, gamma)\n",
    "\n",
    "# convert lists to np arrays and flatten\n",
    "disc_sum_np = np.array(disc_sum)\n",
    "b_estimates_np = np.array(b_estimates)\n",
    "b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "# substract arrays to obtain advantages\n",
    "advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "print(advantages)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogProbs and Ratio Computation\n",
    "\n",
    "We need the ratio of probabilities for an action at state t of the 'new' model vs the old model (maybe because of entropy there is a difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "log old: [[-1.3863043 -1.386295  -1.3862795 -1.3862981]]\n",
      "LogProbs_old first -1.3862972259521484\n"
     ]
    }
   ],
   "source": [
    "# using one Episode as example to get the prob ratio of old vs new\n",
    "logits_old = episodes[0][2]\n",
    "actions = episodes[0][1]\n",
    "obs = episodes[0][0]\n",
    "logits_new = []\n",
    "for i in obs:\n",
    "    tensor = tf.convert_to_tensor(i)\n",
    "    new, action = sample_action(tensor)\n",
    "    logits_new.append(new) \n",
    "\n",
    "\n",
    "#creating oneHot vector with size actions space and getting the log for the probability of choosing this action\n",
    "a = actions[0].numpy()\n",
    "a = a.flatten()\n",
    "print(a)\n",
    "# getting the log\n",
    "logits_old = tf.nn.log_softmax(logits_old[0]) #tryout with only a single action\n",
    "print(f'log old: {logits_old}')\n",
    "logits_old = tf.nn.log_softmax(logits_new)\n",
    "\n",
    "# creating one_hot vectors of the chosen actions for old and new actor logits\n",
    "logprobability_old = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logits_old, axis=1\n",
    "    )\n",
    "logprobability_new = tf.reduce_sum(\n",
    "        tf.one_hot(actions, num_actions) * logits_new, axis=1\n",
    "    )\n",
    "log_prob_old = logprobability_old[a[0]]\n",
    "# compute the ratio - missing is only that we have old and new probability for an action and not the oneHot\n",
    "\n",
    "# print(f'actions Length {len(actions)} {actions}')\n",
    "print(f'LogProbs_old first {log_prob_old[a[0]]}') # check this out, this is the value we want / make streamlining / check why this is not working above\n",
    "\n",
    "\n",
    "## current problem is that we still get a one_hot and not a single log-prob which we need to compute the ratios of old and new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
