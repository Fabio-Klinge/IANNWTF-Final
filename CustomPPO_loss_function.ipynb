{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Environment\n",
    "import gym\n",
    "# Further support\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    '''\n",
    "    Contains all information the agent collects interacting with the environment.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializes empty lists as storages all observation variables during trajectory\n",
    "        '''\n",
    "        # Saves information about the current state of the agent at each step\n",
    "        self.observations = []\n",
    "\n",
    "        # Saves actions made and rewards achieved\n",
    "        self.actions = []\n",
    "        self.logits = []\n",
    "        self.rewards = []\n",
    "        self.BaselineEstimate = []\n",
    "\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self, observation, action, logits, reward, BaselineEstimate):\n",
    "        '''\n",
    "        Adds given information to the storage.\n",
    "\n",
    "        Args:\n",
    "        observation(obj): information (e.g. pixel values) about current state of agent\n",
    "        action(float): Output of the actor network. Describes the action taken\n",
    "        logits():\n",
    "        reward(floats): Rewards collected by agent\n",
    "        BaselineEstimate():\n",
    "        '''\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) # value of critics network\n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        '''\n",
    "        Append all collect values to episodes list once one episode is finished.\n",
    "        Computes all rewards collected in one episode. Prepares storage for next episode.\n",
    "        '''\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate,\n",
    "             # Get the return of the whole episode \n",
    "             sum(self.rewards))\n",
    "             \n",
    "        # Empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "     \n",
    "    def get_episodes(self):\n",
    "        '''\n",
    "        Returns list containing finished trajectories stored in self.episodes\n",
    "        and the amount of episodes passed.\n",
    "        '''\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    '''\n",
    "    Neural network computing the actions the agent will take\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l = [\n",
    "            # Three Dense Layers with random initial parameters having a standart deviation of 0.01\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            \n",
    "            # Output layer with softmax activation function applied to for neurons.\n",
    "            # Outputs prpobability for each of our for actions \n",
    "            # (Do nothing, fire left orientation engine, fire main engine, fire right orientation engine)\n",
    "            Dense(4, activation=\"softmax\")\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create softmax ouutput.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#####  logits = actor(observation) -> actor must be in capitol, gets instantiated twice, maybe idea is wrong\n",
    "#@tf.function\n",
    "def sample_action(observation):\n",
    "    '''\n",
    "    Calls the actor network with state of the agent and returns the network object + the samnpled action\n",
    "\n",
    "    Args:\n",
    "    observation(): Representation of actors state. Same as x in the call function. \n",
    "    '''\n",
    "    # Create actor object\n",
    "    logits = actor(observation)\n",
    "   # tf.print(type(logits))\n",
    "    # Sample action from the Softmax output of the network\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "   # tf.print(action)\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    '''\n",
    "    Represents the value function of the network. \n",
    "    Input is a certain state and output a float value for that state.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize layer architecture for Actor Network.\n",
    "        '''\n",
    "        # Subclassing API\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l = [\n",
    "            # Three Dense Layers with ReLu activation function\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "\n",
    "            # Output layer with Tanh activation function to get float output value ([-1;1])\n",
    "            # Random initial parameters having a standart deviation of 0.01\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        Iterates input x through network to create tanh output between -1 and 1 \n",
    "        giving input state x a value.\n",
    "\n",
    "        Args:\n",
    "        x(): Network input. Pixel values representing the current state of the agent.\n",
    "        '''\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define Hyperparameters\n",
    "'''\n",
    "\n",
    "# Number of iterations\n",
    "epochs = 1\n",
    "# Leads to ~10 Episodes per epoch, then compute new parameters (smaller batching)\n",
    "steps_per_epoch = 1000 \n",
    "\n",
    "# Learning rate for actor and critic\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-4\n",
    "\n",
    "# Movements in environment (state-space) to collect training data\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "\n",
    "# Parameter to decide how strongly the policy ratio gets clipped therefore how much policy (actor network)\n",
    "#  updates we allow\n",
    "# The selected 0.2 is the number proposed by the original paper by OpenAI\n",
    "clip_ratio = 0.2\n",
    "\n",
    "#\n",
    "target_kl = 0.01\n",
    "\n",
    "\n",
    "# Update weights with Adam optimizer\n",
    "optimizer = Adam()\n",
    "\n",
    "# To toggle displaying of environment\n",
    "render = False\n",
    "\n",
    "# Discount variable for rewards to whey immediate rewards stronger\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 12:47:34.348637: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-01 12:47:34.348758: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Reset all states generated by Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# Get dimensions of state and amount of possible actions (4 for LunarLander-v2)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage object to save observations, actions, rewards etc. during trajectory\n",
    "T = Storage()\n",
    "\n",
    "# initialize actor and critics model\n",
    "observation_input = Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "# Initialize: observation(agent state), \n",
    "# episode return(summed rewards for singe ) and \n",
    "# episode length(amount of steps taken (=frames) before agent finished)\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 271.54it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Agent takes steps in environment according to current policy. Information gets saved to update policy.\n",
    "'''\n",
    "\n",
    "\n",
    "episodes_total = 0\n",
    "# Iteration of whole training process\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Initialize values for return, length and episodes\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Each timestep t of steps_per_epoch (in paper denoted as capital T)\n",
    "    #  allows takes on action in a state and saves the information in storage object\n",
    "    for t in tqdm(range(steps_per_epoch)):\n",
    "\n",
    "        # Toggles displaying of environment\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Reshaping observation to fit as input for Actor network (policy)\n",
    "        observation = observation.reshape(1,-1)\n",
    "\n",
    "        # Obtain action and logits for this observation by our actor\n",
    "        logits, action = sample_action(observation=observation)\n",
    "        \n",
    "        # Take action in environment and obtain the rewards for it\n",
    "        # Variable done represents wether agent has finished \n",
    "        # The last variable would be diagnostic information, not needed for training\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "        # Sum up rewards over this episode and count amount of frames\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the Base-Estimate from the Critics network\n",
    "        base_estimate = critic(observation)\n",
    "\n",
    "        # Store Variables collected in this timestep t\n",
    "        T.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "        # Save the new state of our agent\n",
    "        observation = observation_new\n",
    "        \n",
    "        # check if terminal state is reached in environment\n",
    "        if done:\n",
    "            # Save information about episode\n",
    "            T.conclude_episode()\n",
    "            # Refresh environment and reset return and length value\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # obtain all episodes saved in storage\n",
    "    episodes, amount_episodes = T.get_episodes()\n",
    "\n",
    "  \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes [episode][particular values // 0: Observations, 1: actions, 2: logits, 3, rewards, 4: BaselineEstimates from Critics]\n",
    "#print(episodes[0][4])\n",
    "#print(f'Number of Episodes = {amount_episodes}')\n",
    "\n",
    "\n",
    "\n",
    "### Advantagefunction\n",
    "\n",
    "\n",
    "# for i in b_estimates:\n",
    "#   print(i.numpy())\n",
    "\n",
    "# Discounted sum of rewards\n",
    "# Saves list of all rewards in new variable \n",
    "#rewards = episodes[0][3]\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MIGHT NOT WORK\n",
    "#  output for: discounted_reward([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0.99)\n",
    "#  -> [8.91, 7.920000000000001, 6.930000000000001, 5.94, 4.95, 3.96, 2.9699999999999998, 1.98, 0.99, 0]\n",
    "# \n",
    "#  ###\n",
    "\n",
    "def discounted_reward(rewards, gamma):\n",
    "    '''\n",
    "    weighs all rewards in a way such that immediate rewards have a stronger impact than possible future rewards.\n",
    "\n",
    "    Args:\n",
    "    rewards(list): list of all rewards collected by the agent in episode t (?)\n",
    "    gamma(float): Hyperparameter determining how much future rewards should be weighed in\n",
    "    '''\n",
    "    # To select the next reward\n",
    "    i = 0\n",
    "    discounted_rewards = []\n",
    "\n",
    "    # Iterates through every reward and appends a discounted version to the output\n",
    "    for r in rewards:\n",
    "        disc = 0\n",
    "        for t in rewards[i:-1]:\n",
    "            discount_t = gamma ** t\n",
    "            disc += t * discount_t\n",
    "        i += 1\n",
    "        discounted_rewards.append(disc)\n",
    "\n",
    "    # returns list of discounted rewards.\n",
    "    return discounted_rewards\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.35283260e+02 -2.34935956e+02 -2.33778553e+02 -2.30530271e+02\n",
      " -2.31134611e+02 -2.31665327e+02 -2.30913749e+02 -2.31794131e+02\n",
      " -2.33008851e+02 -2.31881619e+02 -2.31563902e+02 -2.31108577e+02\n",
      " -2.29256318e+02 -2.28340632e+02 -2.26305194e+02 -2.25007255e+02\n",
      " -2.25642241e+02 -2.25227027e+02 -2.23800056e+02 -2.21611360e+02\n",
      " -2.20228909e+02 -2.19945791e+02 -2.19777143e+02 -2.17261652e+02\n",
      " -2.14520389e+02 -2.11413892e+02 -2.09469299e+02 -2.08439508e+02\n",
      " -2.07871874e+02 -2.05125732e+02 -2.04040228e+02 -2.02319820e+02\n",
      " -2.02136934e+02 -2.02197151e+02 -2.00515737e+02 -1.97576560e+02\n",
      " -1.96587467e+02 -1.93456302e+02 -1.91390592e+02 -1.90252301e+02\n",
      " -1.88193467e+02 -1.87124362e+02 -1.84108562e+02 -1.82056672e+02\n",
      " -1.80022864e+02 -1.78774245e+02 -1.76945881e+02 -1.75911674e+02\n",
      " -1.77898696e+02 -1.77353453e+02 -1.74853551e+02 -1.75222053e+02\n",
      " -1.74639334e+02 -1.71780002e+02 -1.69993101e+02 -1.68227590e+02\n",
      " -1.67326396e+02 -1.64657098e+02 -1.63422462e+02 -1.60809907e+02\n",
      " -1.59814817e+02 -1.58160671e+02 -1.56530276e+02 -1.55318305e+02\n",
      " -1.54919576e+02 -1.54391551e+02 -1.54187357e+02 -1.52372272e+02\n",
      " -1.52456236e+02 -1.51665669e+02 -1.53130477e+02 -1.53025519e+02\n",
      " -1.52313987e+02 -1.52653423e+02 -1.52875510e+02 -1.51464211e+02\n",
      " -1.51665301e+02 -1.52088592e+02 -1.51681197e+02 -1.51240702e+02\n",
      " -1.50758291e+02 -1.51034633e+02 -1.51555710e+02 -1.52520512e+02\n",
      " -1.52878245e+02 -1.51410398e+02 -1.49744928e+02 -1.47682573e+02\n",
      " -1.45397038e+02 -1.44598580e+02 -1.44783765e+02 -1.41713956e+02\n",
      " -1.47990559e+02 -1.84023516e+02 -2.08168473e+02 -2.06373400e+02\n",
      " -2.06944050e+02 -2.08204707e+02 -2.10815819e+02 -2.12445454e+02\n",
      " -2.12487099e+02 -2.03387791e+02 -2.04577489e+02 -2.14575587e+02\n",
      " -2.17206373e+02 -2.19180463e+02 -2.20971827e+02 -2.23943460e+02\n",
      " -2.16763407e+02 -2.20136667e+02 -2.21814370e+02 -2.25169413e+02\n",
      " -2.26434592e+02 -2.27534033e+02 -2.29275111e+02 -2.31215125e+02\n",
      " -2.27850865e+02 -2.22105284e+02 -2.17037748e+02 -2.30026870e+02\n",
      " -2.29517670e+02 -2.34786821e+02 -2.38885534e+02 -2.41979489e+02\n",
      " -2.43979315e+02 -2.45399865e+02 -2.41567741e+02 -2.41846550e+02\n",
      " -2.37689395e+02 -2.39736198e+02 -2.42440256e+02 -2.44996928e+02\n",
      " -2.35260844e+02 -2.44773552e+02 -2.42933360e+02 -2.41335498e+02\n",
      " -2.39894338e+02 -2.37487348e+02 -2.35419966e+02 -2.37599341e+02\n",
      " -2.37749871e+02 -2.36068092e+02 -2.07375236e+02 -2.08360821e+02\n",
      " -2.08830124e+02 -2.06059414e+02 -2.05108600e+02 -2.04657036e+02\n",
      " -2.02997577e+02 -2.02583503e+02 -2.03030485e+02 -2.11872965e+02\n",
      " -2.09687196e+02 -2.17690190e+02 -2.18265561e+02 -2.16900143e+02\n",
      " -2.17236547e+02 -2.17725862e+02 -2.16117087e+02 -2.04438701e+02\n",
      " -2.02729393e+02 -2.01039214e+02 -2.00548347e+02 -1.97168859e+02\n",
      " -1.95715132e+02 -1.94200511e+02 -1.95008006e+02 -1.92521989e+02\n",
      " -1.82394216e+02 -1.91262431e+02 -1.91198252e+02 -1.91924994e+02\n",
      " -1.91427022e+02 -1.89876089e+02 -1.90753494e+02 -1.91200917e+02\n",
      " -1.90999801e+02 -1.91708160e+02 -1.89509321e+02 -1.89222326e+02\n",
      " -1.90744034e+02 -1.90578015e+02 -1.90312847e+02 -1.91005143e+02\n",
      " -1.92318623e+02 -1.93160699e+02 -1.93822433e+02 -1.81846018e+02\n",
      " -1.92376988e+02 -1.93239824e+02 -1.94098724e+02 -1.94958545e+02\n",
      " -1.95818856e+02 -1.94032497e+02 -1.84007812e+02 -1.84855178e+02\n",
      " -1.84535268e+02 -1.85817486e+02 -1.95077421e+02 -2.04014700e+02\n",
      " -2.04984854e+02 -2.05952541e+02 -2.07925844e+02 -2.07963733e+02\n",
      " -2.09924517e+02 -2.09752677e+02 -2.00295886e+02 -2.02140354e+02\n",
      " -2.02653415e+02 -2.02180523e+02 -1.92397263e+02 -1.99656137e+02\n",
      " -1.87458825e+02 -1.97019510e+02 -1.97602046e+02 -2.07211424e+02\n",
      " -2.05668221e+02 -2.06797966e+02 -2.08389097e+02 -1.96695985e+02\n",
      " -1.86435957e+02 -1.97329276e+02 -2.07769851e+02 -2.09027697e+02\n",
      " -2.10719413e+02 -2.09313116e+02 -2.00462548e+02 -1.99962288e+02\n",
      " -1.98621603e+02 -2.09524228e+02 -2.10298647e+02 -2.11128209e+02\n",
      " -2.14056386e+02 -2.16711122e+02 -2.06986529e+02 -2.06550472e+02\n",
      " -2.07630290e+02 -2.08061193e+02 -2.18846724e+02 -2.20804289e+02\n",
      " -2.21903552e+02 -2.10116978e+02 -2.12136988e+02 -2.13755426e+02\n",
      " -2.23397648e+02 -2.24446977e+02 -2.25682641e+02 -2.25223841e+02\n",
      " -2.13901179e+02 -2.12240833e+02 -2.00665886e+02 -2.01515943e+02\n",
      " -2.01757389e+02 -2.03216779e+02 -2.04533959e+02 -2.03747783e+02\n",
      " -2.03104012e+02 -2.02478540e+02 -2.01602119e+02 -1.99553054e+02\n",
      " -2.09737203e+02 -2.18018764e+02 -2.17361771e+02 -2.18307219e+02\n",
      " -2.19684941e+02 -2.20499268e+02 -2.22209304e+02 -2.23898607e+02\n",
      " -2.25490687e+02 -2.26119907e+02 -2.26314308e+02 -2.25988421e+02\n",
      " -2.27747004e+02 -2.29393470e+02 -2.28823338e+02 -2.29703342e+02\n",
      " -2.28706252e+02 -2.28883469e+02 -2.29846825e+02 -2.28890293e+02\n",
      " -2.04113642e+02 -2.04622463e+02 -2.04989357e+02 -2.04599575e+02\n",
      " -2.04731884e+02 -2.13809410e+02 -2.22640711e+02 -2.24247600e+02\n",
      " -2.25184957e+02 -2.24282429e+02 -2.25368671e+02 -2.26761497e+02\n",
      " -2.27045393e+02 -2.27334866e+02 -2.27895854e+02 -2.27032702e+02\n",
      " -2.28123158e+02 -2.26512850e+02 -2.03859971e+02 -2.13056466e+02\n",
      " -2.13014184e+02 -2.12044003e+02 -2.12011784e+02 -2.13638322e+02\n",
      " -2.12828975e+02 -2.12956335e+02 -2.00365894e+02 -2.09280607e+02\n",
      " -2.17838597e+02 -2.18176006e+02 -2.17593033e+02 -2.17931737e+02\n",
      " -2.19406206e+02 -2.21905589e+02 -2.12040902e+02 -2.13704248e+02\n",
      " -2.13879250e+02 -2.14590917e+02 -2.04152277e+02 -2.04929780e+02\n",
      " -2.04776265e+02 -2.05293019e+02 -2.14498276e+02 -2.26245200e+02\n",
      " -2.23483325e+02 -1.99722009e+02 -1.99089572e+02 -2.00500940e+02\n",
      " -2.00406986e+02 -1.98169867e+02 -2.17177016e+02 -2.19137507e+02\n",
      " -2.18184380e+02 -2.16511883e+02 -2.17681637e+02 -2.16584739e+02\n",
      " -2.15458449e+02 -2.16067552e+02 -2.15210681e+02 -2.15217702e+02\n",
      " -2.13668504e+02 -1.87876597e+02 -1.88206036e+02 -1.87791294e+02\n",
      " -1.86166904e+02 -1.86187770e+02 -1.85352586e+02 -1.83879303e+02\n",
      " -1.94712953e+02 -1.95638678e+02 -2.05410956e+02 -2.04534299e+02\n",
      " -2.02675330e+02 -1.89217902e+02 -1.80145466e+02 -1.80481920e+02\n",
      " -1.81745960e+02 -1.79494885e+02 -1.79945587e+02 -1.79684630e+02\n",
      " -1.80737198e+02 -1.78666195e+02 -1.76249598e+02 -1.76346067e+02\n",
      " -1.74117064e+02 -1.76504571e+02 -1.77014826e+02 -1.78413483e+02\n",
      " -1.80413868e+02 -1.79233745e+02 -1.79665385e+02 -1.78620455e+02\n",
      " -1.77295053e+02 -1.76941137e+02 -1.74649023e+02 -1.73616021e+02\n",
      " -1.71072067e+02 -1.69262591e+02 -1.83903269e+02 -1.75113796e+02\n",
      " -1.88183895e+02 -1.95119872e+02 -1.96397662e+02 -1.98047515e+02\n",
      " -1.98517874e+02 -1.93454255e+02 -1.87530719e+02 -1.90246747e+02\n",
      " -1.93045248e+02 -1.71914046e+02 -1.73598905e+02 -1.75029670e+02\n",
      " -1.74420783e+02 -1.72645030e+02 -1.74029700e+02 -1.73676571e+02\n",
      " -1.71126118e+02 -1.68224830e+02 -1.79975095e+02 -1.82234460e+02\n",
      " -1.80407115e+02 -1.87839602e+02 -1.87368051e+02 -1.88676315e+02\n",
      " -1.90507159e+02 -1.88878776e+02 -1.89618071e+02 -1.87845012e+02\n",
      " -1.88825607e+02 -1.79311352e+02 -1.79305128e+02 -1.78676829e+02\n",
      " -1.77824613e+02 -1.88561847e+02 -1.90826747e+02 -1.90494604e+02\n",
      " -1.91789155e+02 -1.91317071e+02 -1.91003223e+02 -1.90644344e+02\n",
      " -1.91943131e+02 -1.91279982e+02 -1.90307797e+02 -1.89697659e+02\n",
      " -1.89134901e+02 -1.89636059e+02 -1.89306799e+02 -1.88697011e+02\n",
      " -1.89241668e+02 -1.90648034e+02 -1.90491186e+02 -1.90948244e+02\n",
      " -1.90262489e+02 -1.90628497e+02 -1.86517004e+02 -1.56028486e+02\n",
      " -1.58931720e+02 -1.61088973e+02 -1.56549992e+02 -1.59024900e+02\n",
      " -1.61160251e+02 -1.57855832e+02 -1.59294362e+02 -1.58173039e+02\n",
      " -1.56767456e+02 -1.58080197e+02 -1.56138978e+02 -1.59185941e+02\n",
      " -1.57304933e+02 -1.55106789e+02 -1.58556889e+02 -1.55714814e+02\n",
      " -1.64379014e+02 -1.74889881e+02 -1.75103142e+02 -1.70819404e+02\n",
      " -1.49199119e+02 -1.50301826e+02 -1.49661798e+02 -1.59180039e+02\n",
      " -1.58823025e+02 -1.61980537e+02 -1.60161803e+02 -1.61211035e+02\n",
      " -1.59343911e+02 -1.69568706e+02 -1.70509852e+02 -1.66248858e+02\n",
      " -1.68040128e+02 -1.58951017e+02 -1.58715033e+02 -1.68083956e+02\n",
      " -1.67444765e+02 -1.66877934e+02 -1.68149401e+02 -1.67628390e+02\n",
      " -1.67838945e+02 -1.68037198e+02 -1.67666258e+02 -1.67019740e+02\n",
      " -1.65740943e+02 -1.66311556e+02 -1.65639302e+02 -1.54684119e+02\n",
      " -1.64958638e+02 -1.64679908e+02 -1.65242169e+02 -1.65036525e+02\n",
      " -1.64800557e+02 -1.65363202e+02 -1.64976012e+02 -1.62009361e+02\n",
      " -1.53823532e+02 -1.50964660e+02 -1.42005876e+02 -1.49761272e+02\n",
      " -1.49111183e+02 -1.48008739e+02 -1.58672194e+02 -1.60380038e+02\n",
      " -1.59799052e+02 -1.60354176e+02 -1.60438784e+02 -1.59811342e+02\n",
      " -1.57758733e+02 -1.46421378e+02 -1.54084084e+02 -1.54624191e+02\n",
      " -1.55223127e+02 -1.56516904e+02 -1.54094102e+02 -1.30468073e+02\n",
      " -1.29671911e+02 -1.47737913e+02 -1.48579679e+02 -1.48541211e+02\n",
      " -1.45405660e+02 -1.28716273e+02 -1.14594686e+02 -1.16969294e+02\n",
      " -1.11762586e+02 -1.06417122e+02 -1.08304043e+02 -1.10798793e+02\n",
      " -1.12718123e+02 -1.14342460e+02 -1.14614882e+02 -1.14750033e+02\n",
      " -1.15470986e+02 -1.14383152e+02 -1.13428066e+02 -1.12195755e+02\n",
      " -1.10201140e+02 -1.07351666e+02 -1.08046329e+02 -1.11199875e+02\n",
      " -1.12077639e+02 -1.14054994e+02 -1.11911950e+02 -1.22021066e+02\n",
      " -1.29280995e+02 -1.28662373e+02 -1.33769486e+02 -1.34183104e+02\n",
      " -1.35065186e+02 -1.34270698e+02 -1.34021789e+02 -1.34867621e+02\n",
      " -1.35452577e+02 -1.37959865e+02 -1.39511687e+02 -1.39548885e+02\n",
      " -1.39229949e+02 -1.38819576e+02 -1.35920366e+02 -1.27326568e+02\n",
      " -1.15773525e+02 -1.13605766e+02 -1.14117405e+02 -1.12305893e+02\n",
      " -1.21053255e+02 -1.31439288e+02 -1.33401498e+02 -1.29375739e+02\n",
      " -1.08574343e+02 -1.07849267e+02 -1.16556560e+02 -1.13540803e+02\n",
      " -9.75468717e+01 -9.91712227e+01 -1.01515930e+02 -1.02206774e+02\n",
      " -9.97725948e+01 -9.73657057e+01 -1.08108968e+02 -1.08605635e+02\n",
      " -1.09548981e+02 -9.72482298e+01 -1.04330827e+02 -1.05059201e+02\n",
      " -1.16887401e+02 -1.19427280e+02 -1.14651844e+02 -1.14677516e+02\n",
      " -1.11395514e+02 -1.01982426e+02 -1.01951479e+02 -9.92766246e+01\n",
      " -8.97766029e+01 -9.06331679e+01 -8.94244039e+01 -9.89958121e+01\n",
      " -9.78139216e+01 -9.81665382e+01 -1.08424264e+02 -1.12697385e+02\n",
      " -1.08588419e+02 -1.09100603e+02 -1.10776716e+02 -1.12847534e+02\n",
      " -1.00276142e+02 -1.01126457e+02 -9.95585762e+01 -1.09528210e+02\n",
      " -9.88249900e+01 -1.05916718e+02 -9.64824958e+01 -9.58224437e+01\n",
      " -1.05691820e+02 -1.05246270e+02 -1.06619290e+02 -1.06163534e+02\n",
      " -1.05532073e+02 -1.05327371e+02 -9.23161561e+01 -1.01707443e+02\n",
      " -9.98547813e+01 -1.01306181e+02 -1.02844687e+02 -1.00409871e+02\n",
      " -1.02257962e+02 -1.02600708e+02 -1.02092405e+02 -1.01552040e+02\n",
      " -1.02109342e+02 -1.01901662e+02 -1.01313681e+02 -1.02326468e+02\n",
      " -9.79002605e+01 -6.97171805e+01 -7.22186959e+01 -7.42267856e+01\n",
      " -7.37523768e+01 -7.30297438e+01 -7.14944567e+01 -7.06595551e+01\n",
      " -6.86858218e+01 -6.52999138e+01 -8.26714161e+01 -8.56151951e+01\n",
      " -8.21415263e+01 -8.34578287e+01 -7.32922670e+01 -7.45060696e+01\n",
      " -7.33983668e+01 -7.14478917e+01 -8.18453442e+01 -8.44362705e+01\n",
      " -8.49470079e+01 -8.40507995e+01 -8.31693065e+01 -6.16816481e+01\n",
      " -7.12851954e+01 -6.97328795e+01 -8.05375120e+01 -8.05939940e+01\n",
      " -8.03131400e+01 -7.97424235e+01 -7.84430005e+01 -6.70544190e+01\n",
      " -7.59474093e+01 -7.74348245e+01 -7.78634020e+01 -7.72176532e+01\n",
      " -7.37661070e+01 -4.50164650e+01 -4.66210236e+01 -4.79132787e+01\n",
      " -4.74921051e+01 -4.62421584e+01 -4.32462612e+01 -5.92555392e+01\n",
      " -6.29417921e+01 -6.31797414e+01 -6.43803135e+01 -6.21365950e+01\n",
      " -6.28001414e+01 -6.02478563e+01 -3.20467676e+01 -3.43922529e+01\n",
      " -3.65977513e+01 -3.59355125e+01 -3.16827086e+01 -2.63584662e+01\n",
      " -2.88787415e+01 -2.50791093e+01 -2.71680214e+01 -2.94811423e+01\n",
      " -2.86544234e+01 -3.08471764e+01 -3.20966584e+01 -3.02432777e+01\n",
      " -2.83038561e+01 -2.70536625e+01 -2.58796651e+01 -2.30608047e+01\n",
      " -2.42601033e+01 -2.42494357e+01 -2.17150103e+01 -2.25373393e+01\n",
      " -2.26462479e+01 -2.00206074e+01 -1.68729911e+01 -1.52336591e+01\n",
      " -1.51913527e+01 -2.79375366e+01 -2.87294773e+01 -2.87404563e+01\n",
      " -2.96330067e+01 -3.88191302e+01 -3.87379579e+01 -3.94668270e+01\n",
      " -4.02382222e+01 -4.37780877e+01 -4.37443608e+01 -4.32842656e+01\n",
      " -4.05187694e+01 -4.31871083e+01 -4.50520933e+01 -2.98564085e+01\n",
      " -2.26734385e+01 -2.54453788e+01 -2.58897020e+01 -2.47983273e+01\n",
      " -2.31900855e+01 -2.39645977e+01 -2.33953707e+01 -2.17777342e+01\n",
      " -2.17470643e+01 -3.22140319e+01 -3.20547631e+01 -4.16147502e+01\n",
      " -4.07347445e+01 -4.00188217e+01 -4.14498158e+01 -4.31535264e+01\n",
      " -4.39720392e+01 -3.91967968e+01 -3.60847803e+01 -1.02978555e+01\n",
      " -1.17049312e+01 -1.24576212e+01 -1.32405014e+01 -2.15149535e+01\n",
      " -2.20456704e+01 -9.61944836e+00 -9.38339603e+00 -8.27592761e+00\n",
      " -7.18248126e+00 -9.04077581e+00 -6.87132752e+00 -2.23900949e+01\n",
      " -2.34961428e+01 -2.75051275e+01 -2.86533237e+01 -2.79675589e+01\n",
      " -2.50071639e+01 -2.63408792e+01 -1.66602717e+01 -1.33110487e+01\n",
      " -4.45020503e+00 -1.76340685e+00 -3.35620624e+00 -2.14581700e+00\n",
      " -2.20876950e+00  1.34110621e-06]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Computes advantages.\n",
    "'''\n",
    "\n",
    "# Saves list of all rewards in new variable \n",
    "rewards = episodes[0][3]\n",
    "# Get discounted sum of rewards \n",
    "disc_sum = discounted_reward(rewards, gamma)\n",
    "\n",
    "\n",
    "# Estimated Value of the current situtation from the critics network\n",
    "b_estimates = episodes[0][4] \n",
    "\n",
    "# Convert lists to np arrays and flatten\n",
    "disc_sum_np = np.array(disc_sum)\n",
    "b_estimates_np = np.array(b_estimates)\n",
    "b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "# substract arrays to obtain advantages\n",
    "advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "print(advantages)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogProbs and Ratio Computation\n",
    "\n",
    "We need the ratio of probabilities for an action at state t of the 'new' model vs the old model (maybe because of entropy there is a difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log old: [[0.24999921 0.24999891 0.24999474 0.25000718]]\n",
      "log new: [[0.24999921 0.24999891 0.24999474 0.25000718]]\n",
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#creating oneHot vector with size actions space and getting the log for the probability of choosing this action\n",
    "\n",
    "print(f'log old: {logits_old}')\n",
    "print(f'log new: {logits_new}')\n",
    "\n",
    "\n",
    "### this function currently only takes one single action and 2 sets of logits and computes the ratio of that\n",
    "\n",
    "def get_ratio(action, logits_old, logits_new):\n",
    "\n",
    "    #get the Logarithmic version of all logits for computational efficiency\n",
    "    log_prob_old = tf.nn.log_softmax(logits_old)\n",
    "    log_prob_new = tf.nn.log_softmax(logits_new)\n",
    "\n",
    "    # encode in OneHotVector and reduce to sum, giving the log_prob for the action the agent took for both policies\n",
    "    logprobability_old = tf.reduce_sum(\n",
    "        tf.one_hot(action, num_actions) * log_prob_old, axis=1\n",
    "    )\n",
    "    logprobability_new = tf.reduce_sum(\n",
    "        tf.one_hot(action, num_actions) * log_prob_new, axis=1\n",
    "    )\n",
    "    # get the ratio of new over old prob\n",
    "    ratio = tf.exp(logprobability_new - logprobability_old)\n",
    "\n",
    "    print(ratio)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# using one Episode as example to get the prob ratio of old vs new\n",
    "logits_old = episodes[0][2][0]\n",
    "action = episodes[0][1][0].numpy()\n",
    "obs = episodes[0][0]\n",
    "logits_new = []\n",
    "for i in obs:\n",
    "    tensor = tf.convert_to_tensor(i)\n",
    "    new, _ = sample_action(tensor)\n",
    "    logits_new.append(new) \n",
    "\n",
    "logits_new = logits_new[0]\n",
    "\n",
    "get_ratio(action, logits_old, logits_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
