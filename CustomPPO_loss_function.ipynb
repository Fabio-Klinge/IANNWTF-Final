{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import scipy.signal\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrajectoryStorage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self):\n",
    "        # init creating a storage for all observation variables during trajectory\n",
    "        self.observations = []\n",
    "        # create arrays for chosen actions and rewards\n",
    "        self.actions = []\n",
    "        self.logits = []\n",
    "        self.rewards = []\n",
    "        self.BaselineEstimate = []\n",
    "        # finished episodes will be completely stored in this list \n",
    "        self.episodes = []\n",
    "\n",
    "\n",
    "    def store(self,observation, action, logits, reward, BaselineEstimate):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.logits.append(logits)\n",
    "        self.rewards.append(reward)\n",
    "        self.BaselineEstimate.append(BaselineEstimate) # value of critics network\n",
    "        \n",
    "\n",
    "    def conclude_episode(self):\n",
    "        # append all already stored values to finished episodes\n",
    "        self.episodes.append(\n",
    "            [self.observations,\n",
    "             self.actions, \n",
    "             self.logits,\n",
    "             self.rewards,\n",
    "             self.BaselineEstimate, \n",
    "             sum(self.rewards)]) # get the return of the whole episode\n",
    "             \n",
    "        # empty the arrays for new trajectory\n",
    "        self.observations.clear()\n",
    "        # create arrays for chosen actions and rewards\n",
    "        self.actions.clear()\n",
    "        self.logits.clear()\n",
    "        self.rewards.clear()\n",
    "        self.BaselineEstimate.clear()\n",
    "\n",
    "\n",
    "    # return array of finished trajectories stored in self.episodes and the amount of episodes\n",
    "    def get_episodes(self):\n",
    "        return self.episodes, len(self.episodes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "\n",
    "        self.l = [\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(4, activation=\"softmax\")\n",
    "        ]\n",
    "\n",
    "    #@tf.function        \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "\n",
    "#@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "   # tf.print(type(logits))\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "   # tf.print(action)\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l = [\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer\n",
    "            (stddev=0.01)),\n",
    "            Dense(1, activation=\"tanh\", kernel_regularizer=tf.random_normal_initializer(stddev=0.01))\n",
    "        ]\n",
    "\n",
    "    #@tf.function \n",
    "    def call(self, x):\n",
    "        for l in self.l:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Hyperparameters\n",
    "epochs = 1\n",
    "steps_per_epoch = 1000 # ~10 Episodes per epoch, then compute new parameters (smaller batching)\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 3e-4\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "clip_ratio = 0.2\n",
    "target_kl = 0.01\n",
    "optimizer = Adam()\n",
    "\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# define environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# get observation_dims and amount of possible actions (1 for CartPole-v1)\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# create Storage for observations, actions, rewards etc during trajectory\n",
    "T = Storage()\n",
    "\n",
    "# init the actor and critics model\n",
    "observation_input = Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 297.80it/s]\n"
     ]
    }
   ],
   "source": [
    "episodes_total = 0\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    for t in tqdm(range(steps_per_epoch)):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        observation = observation.reshape(1,-1)\n",
    "\n",
    "        # obtain action and logits for this observation by our actor\n",
    "        logits, action = sample_action(observation=observation)\n",
    "        \n",
    "        # make a step in environment and obtain the rewards for it\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "        # sum up rewards over this episode and count amount of frames\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # get the Base-Estimate from the Critics network\n",
    "        base_estimate = critic(observation)\n",
    "\n",
    "        # store Variables collected in this step\n",
    "        T.store(observation=observation, action=action, logits=logits, reward=reward, BaselineEstimate=base_estimate)\n",
    "        #update the observations\n",
    "        observation = observation_new\n",
    "        # check if terminal state is reached in env, if so save episode and refresh storage, reset env\n",
    "        if done:\n",
    "            T.conclude_episode()\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # obtain all episodes saved in storage\n",
    "    episodes, amount_episodes = T.get_episodes()\n",
    "\n",
    "  \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Episodes = 11\n",
      "[-0.023313775499132133, 1.1764470098363813, 1.711304128904062, 1.6116477699257825, 1.1936177758052509, 0.30419220512476497, 0.9673390278992144, -0.23056043074254148, -0.4466325746305688, -0.710873833428791, -1.0975583846965435, -3.7870338066730254, 0.08511483214221016, -2.9944370794186055]\n"
     ]
    }
   ],
   "source": [
    "  # episodes [episode][particular values // 0: Observations, 1: actions, 2: logits, 3, rewards, 4: BaselineEstimates from Critics]\n",
    "  #print(episodes[0][4])\n",
    "  print(f'Number of Episodes = {amount_episodes}')\n",
    "\n",
    "\n",
    "\n",
    "  ### Advantagefunction\n",
    "\n",
    "  # estimated Value of the current situtation from the critics network\n",
    "  b_estimates = episodes[0][4] \n",
    "  # for i in b_estimates:\n",
    "  #   print(i.numpy())\n",
    "\n",
    "  # Discounted sum of rewards\n",
    "  print(episodes[0][3]) \n",
    "  rewards = episodes[0][3]\n",
    "  gamma = 0.99\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward(rewards, gamma):\n",
    "    i = 0\n",
    "    discounted_rewards = []\n",
    "    for r in rewards:\n",
    "        disc = 0\n",
    "        for t in rewards[i:-1]:\n",
    "            discount_t = gamma ** t\n",
    "            disc += t * discount_t\n",
    "        i += 1\n",
    "        discounted_rewards.append(disc)\n",
    "    return discounted_rewards\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.93418938e-01  5.16735853e-01 -6.45881934e-01 -2.32800279e+00\n",
      " -3.91375530e+00 -5.09314033e+00 -5.39640611e+00 -6.35438628e+00\n",
      " -6.12329431e+00 -5.67465357e+00 -4.95868324e+00 -3.84895013e+00\n",
      "  8.50003914e-02 -4.10063149e-05]\n"
     ]
    }
   ],
   "source": [
    "# get discounted sum of rewards \n",
    "disc_sum = discounted_reward(rewards, gamma)\n",
    "\n",
    "# convert lists to np arrays and flatten\n",
    "disc_sum_np = np.array(disc_sum)\n",
    "b_estimates_np = np.array(b_estimates)\n",
    "b_estimates_np = b_estimates_np.flatten()\n",
    "\n",
    "# substract arrays to obtain advantages\n",
    "advantages = np.subtract(disc_sum_np, b_estimates_np)\n",
    "print(advantages)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
